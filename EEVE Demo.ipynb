{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF \\\n",
    "    ggml-model-Q5_K_M.gguf \\\n",
    "    --local-dir ./EEVE-Korean-Instruct-10.8B-v1.0-GGUF \\\n",
    "    --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# download ollama\n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "# execute ollama server in background\n",
    "!nohup ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ollama create EEVE-Korean-10.8B -f ./EEVE-Korean-Instruct-10.8B-v1.0-GGUF/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# install langchain-ollama and ignore stdout or stderr by redirecting /dev/null\n",
    "!pip install -U langchain-ollama > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\")\n",
    "\n",
    "# Prompt setting\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # user's question\n",
    "    {\"role\": \"user\", \"content\": \"한글을 창제한 사람은 누구야?\"}, \n",
    "]\n",
    "\n",
    "# assistant's answer\n",
    "response = chain.invoke({\"messages\": messages})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "assistant_answer = {\"role\": \"assistant\", \"content\": response}\n",
    "messages.append(assistant_answer)\n",
    "\n",
    "another_question = {\"role\": \"user\", \"content\": \"내가 무엇에 대해 질문했어?\"}\n",
    "messages.append(another_question)\n",
    "\n",
    "response = chain.invoke({\"messages\": messages})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
